{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "Using word embeddings to (hopefully) improve prediction accuracy of our property listing classification data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cleaning import process_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_json('property_descriptions.json')\n",
    "df['description'] = df['description'].apply(process_text)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['description'], df['advertiser'], test_size=0.3)\n",
    "sentences = []\n",
    "for descriptions in x_train:\n",
    "    sentences.append(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model.\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=4, size=300, \n",
    "                          min_count = 40, window = 10, sample = 1e-3)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging vectors\n",
    "An option for utilising word2Vec is by averaging the word vectors within each sample of text. Pretty basic but worth checking out how well this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.75      0.83      0.79      2625\n",
      "   flatmate       0.70      0.73      0.71      2673\n",
      "   landlord       0.66      0.58      0.61      3043\n",
      "\n",
      "avg / total       0.70      0.70      0.70      8341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def average_text(text):\n",
    "    \"\"\"Use trained word2vec model to average property descriptions\"\"\"\n",
    "    text = text.values\n",
    "    text_matrix = np.zeros((text.shape[0], 300))\n",
    "    for i in range(text.shape[0]):\n",
    "        text_sum = 0\n",
    "        count = 0\n",
    "        words = text[i]\n",
    "        for word in words:\n",
    "            try:\n",
    "                text_sum += model.wv.get_vector(word)\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if count != 0:\n",
    "            text_matrix[i,:] = text_sum / count\n",
    "        else:\n",
    "            text_matrix[i,:] = np.zeros(300)\n",
    "    return text_matrix\n",
    "            \n",
    "x_train = average_text(x_train)\n",
    "x_test = average_text(x_test)\n",
    "\n",
    "# Train SVC with average vectors.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train, y_train)\n",
    "pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cal_lamont/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:359: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from cleaning import process_text\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags advertiser')\n",
    "\n",
    "# df = pd.read_json('property_descriptions.json')\n",
    "# df['description'] = df['description'].apply(process_text)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['description'], df['advertiser'], test_size=0.3)\n",
    "sentences = []\n",
    "count = 0\n",
    "for listing in df.index:\n",
    "    sentences.append(SentimentDocument(df.loc[listing,'description'], [count], df.loc[listing,'advertiser']))\n",
    "    count += 1 \n",
    "# Train a number of samples to compare performance.    \n",
    "models = [\n",
    "    doc2vec.Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=4),\n",
    "    doc2vec.Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=4),\n",
    "    doc2vec.Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=4)\n",
    "]\n",
    "# model = doc2vec.Doc2Vec(size=100, window=4, min_count=5, workers=4)\n",
    "models[0].build_vocab(sentences)\n",
    "for model in models[1:]:\n",
    "    model.reset_from(models[0])\n",
    "    \n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name = {}\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([models[1], models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([models[1], models[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for epoch 0: 22.2952082157135\n",
      "Time elapsed for epoch 1: 20.056252002716064\n",
      "Time elapsed for epoch 2: 20.74404788017273\n",
      "Time elapsed for epoch 3: 21.983181953430176\n",
      "Time elapsed for epoch 4: 19.1989848613739\n",
      "Time elapsed for epoch 5: 18.016569137573242\n",
      "Time elapsed for epoch 6: 19.140444040298462\n",
      "Time elapsed for epoch 7: 20.517252683639526\n",
      "Time elapsed for epoch 8: 17.826881885528564\n",
      "Time elapsed for epoch 9: 16.0154869556427\n",
      "Time elapsed for epoch 10: 16.075623989105225\n",
      "Time elapsed for epoch 11: 15.684322118759155\n",
      "Time elapsed for epoch 12: 17.384845733642578\n",
      "Time elapsed for epoch 13: 19.05274224281311\n",
      "Time elapsed for epoch 14: 18.004001140594482\n",
      "Time elapsed for epoch 15: 16.94865369796753\n",
      "Time elapsed for epoch 16: 16.84950089454651\n",
      "Time elapsed for epoch 17: 19.627247095108032\n",
      "Time elapsed for epoch 18: 17.98558497428894\n",
      "Time elapsed for epoch 19: 19.396530866622925\n"
     ]
    }
   ],
   "source": [
    "# Load existing model or train new model\n",
    "# model = Doc2Vec.load('./imdb.d2v')\n",
    "import time \n",
    "\n",
    "passes = 20\n",
    "alpha = 0.025\n",
    "min_alpha = 0.001\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "from numpy.random import shuffle\n",
    "# Manually run through each epoch to shuffle data for building.\n",
    "for epoch in range(passes):\n",
    "    start = time.time()\n",
    "    shuffle(sentences)\n",
    "    for train_model in models:\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        train_model.train(sentences, total_examples=len(sentences), epochs=1)\n",
    "    alpha -= alpha_delta\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    print(f\"Time elapsed for epoch {epoch}: {elapsed}\")\n",
    "    \n",
    "# model.save('./listing_model.d2v')\n",
    "# model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.60      0.71      0.65      2144\n",
      "   flatmate       0.55      0.55      0.55      2261\n",
      "   landlord       0.52      0.45      0.48      2545\n",
      "\n",
      "avg / total       0.56      0.56      0.56      6950\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.77      0.84      0.80      2144\n",
      "   flatmate       0.74      0.73      0.73      2261\n",
      "   landlord       0.68      0.63      0.66      2545\n",
      "\n",
      "avg / total       0.73      0.73      0.73      6950\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.62      0.72      0.67      2144\n",
      "   flatmate       0.61      0.56      0.58      2261\n",
      "   landlord       0.55      0.52      0.53      2545\n",
      "\n",
      "avg / total       0.59      0.59      0.59      6950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on 3/4 of the data\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "idxs = np.random.permutation(range(len(sentences)))\n",
    "train_idxs = list(idxs[len(sentences)//4:])\n",
    "test_idxs = list(idxs[:len(sentences)//4])\n",
    "\n",
    "for model in models:\n",
    "    test_regressors = [model.infer_vector(doc.words, steps=3, alpha=0.1) for doc in sentences]\n",
    "    train_targets, train_vectors = zip(*[(sentences[idx].advertiser, test_regressors[idx]) for idx in train_idxs])\n",
    "    test_targets, test_vectors = zip(*[(sentences[idx].advertiser, test_regressors[idx]) for idx in test_idxs])\n",
    "    \n",
    "    clf = svm.LinearSVC()\n",
    "#     clf = LogisticRegression()\n",
    "    clf.fit(train_vectors, train_targets)\n",
    "    pred = clf.predict(test_vectors)\n",
    "    print(classification_report(test_targets, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('students', 0.6108071804046631), ('researcher', 0.5657050013542175), ('studentsno', 0.5259338617324829), ('dancer', 0.4997761845588684), ('doctor', 0.4835510849952698), ('worker', 0.4834507703781128), ('malbec', 0.4711993336677551), ('clown', 0.4616277813911438), ('female', 0.44724586606025696), ('graduate', 0.44703590869903564)]\n",
      "\n",
      "[('just', 0.40552806854248047), ('brussels', 0.3544454574584961), ('mirrorshared', 0.35267797112464905), ('ava', 0.34928223490715027), ('cinemas', 0.3465655744075775), ('very', 0.34344610571861267), ('raised', 0.3312017321586609), ('contribute', 0.32958996295928955), ('contractdeposit', 0.32904717326164246), ('routines', 0.3278307318687439)]\n",
      "\n",
      "[('students', 0.5872292518615723), ('graduate', 0.5242910385131836), ('couple', 0.5180780291557312), ('worker', 0.4828130006790161), ('studying', 0.471962571144104), ('postgraduate', 0.45938220620155334), ('woman', 0.4401608109474182), ('peoplesingle', 0.42400458455085754), ('researcher', 0.423648476600647), ('studentscouples', 0.41380536556243896)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model.wv.most_similar(\"student\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.65      0.76      0.70      2197\n",
      "   flatmate       0.62      0.60      0.61      2203\n",
      "   landlord       0.57      0.51      0.54      2550\n",
      "\n",
      "avg / total       0.61      0.62      0.61      6950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train SVC with average vectors.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# clf = svm.LinearSVC()\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vectors, train_targets)\n",
    "pred = clf.predict(test_vectors)\n",
    "print(classification_report(test_targets, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't seem to be performing that well so let's have a closer inspection of it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
