{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "Using word embeddings to (hopefully) improve prediction accuracy of our property listing classification data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cleaning import process_text\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_json('property_descriptions.json')\n",
    "df['description'] = df['description'].apply(process_text)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['description'], df['advertiser'], test_size=0.3)\n",
    "sentences = []\n",
    "for descriptions in x_train:\n",
    "    sentences.append(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model.\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences, workers=4, size=300, \n",
    "                          min_count = 40, window = 10, sample = 1e-3)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging vectors\n",
    "An option for utilising word2Vec is by averaging the word vectors within each sample of text. Pretty basic but worth checking out how well this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.75      0.83      0.79      2625\n",
      "   flatmate       0.70      0.73      0.71      2673\n",
      "   landlord       0.66      0.58      0.61      3043\n",
      "\n",
      "avg / total       0.70      0.70      0.70      8341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def average_text(text):\n",
    "    \"\"\"Use trained word2vec model to average property descriptions\"\"\"\n",
    "    text = text.values\n",
    "    text_matrix = np.zeros((text.shape[0], 300))\n",
    "    for i in range(text.shape[0]):\n",
    "        text_sum = 0\n",
    "        count = 0\n",
    "        words = text[i]\n",
    "        for word in words:\n",
    "            try:\n",
    "                text_sum += model.wv.get_vector(word)\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if count != 0:\n",
    "            text_matrix[i,:] = text_sum / count\n",
    "        else:\n",
    "            text_matrix[i,:] = np.zeros(300)\n",
    "    return text_matrix\n",
    "            \n",
    "x_train = average_text(x_train)\n",
    "x_test = average_text(x_test)\n",
    "\n",
    "# Train SVC with average vectors.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(x_train, y_train)\n",
    "pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cal_lamont/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py:359: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "from cleaning import process_text_doc2vec\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags advertiser')\n",
    "\n",
    "df = pd.read_json('property_descriptions.json')\n",
    "df['description'] = df['description'].apply(process_text_doc2vec)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(df['description'], df['advertiser'], test_size=0.3)\n",
    "sentences = []\n",
    "count = 0\n",
    "for listing in df.index:\n",
    "    for sent in df.loc[listing,'description']:\n",
    "        sentences.append(SentimentDocument(sent, [count], df.loc[listing, 'advertiser']))\n",
    "        count += 1\n",
    "        \n",
    "model = doc2vec.Doc2Vec(size=100, window=10, min_count=5, workers=4)\n",
    "model.build_vocab(sentences)\n",
    "model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 3/4 of the data\n",
    "import numpy as np\n",
    "idxs = np.random.permutation(range(len(sentences)))\n",
    "train_idxs = list(idxs[len(sentences)//4:])\n",
    "test_idxs = list(idxs[:len(sentences)//4])\n",
    "\n",
    "train_targets, train_vectors = zip(*[(sentences[idx].advertiser, model.docvecs[idx]) for idx in train_idxs])\n",
    "test_targets, test_vectors = zip(*[(sentences[idx].advertiser, model.docvecs[idx]) for idx in test_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      agent       0.00      0.00      0.00     16165\n",
      "   flatmate       0.35      0.01      0.01     23776\n",
      "   landlord       0.40      0.99      0.57     26161\n",
      "\n",
      "avg / total       0.28      0.40      0.23     66102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cal_lamont/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Train SVC with average vectors.\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "clf.fit(train_vectors, train_targets)\n",
    "pred = clf.predict(test_vectors)\n",
    "print(classification_report(test_targets, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent = [sent.words for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['no',\n",
       "  'admin',\n",
       "  'fees',\n",
       "  'come',\n",
       "  'and',\n",
       "  'view',\n",
       "  'this',\n",
       "  'newly',\n",
       "  'refurbished',\n",
       "  'spacious',\n",
       "  'house',\n",
       "  'whilst',\n",
       "  'rooms',\n",
       "  'are',\n",
       "  'still',\n",
       "  'available',\n",
       "  'the',\n",
       "  'benefits',\n",
       "  'of',\n",
       "  'the',\n",
       "  'property',\n",
       "  'include',\n",
       "  'modern',\n",
       "  'and',\n",
       "  'clean',\n",
       "  'furnishings',\n",
       "  'all',\n",
       "  'inclusive',\n",
       "  'bills',\n",
       "  'high',\n",
       "  'speed',\n",
       "  'unlimited',\n",
       "  'fiber',\n",
       "  'optic',\n",
       "  'broadband',\n",
       "  'weekly',\n",
       "  'cleaning',\n",
       "  'throughout',\n",
       "  'communal',\n",
       "  'areas',\n",
       "  'walking',\n",
       "  'distance',\n",
       "  'from',\n",
       "  'convenience',\n",
       "  'stores',\n",
       "  'and',\n",
       "  'supermarkets',\n",
       "  'professional',\n",
       "  'management',\n",
       "  'and',\n",
       "  'repairs',\n",
       "  'procedures',\n",
       "  'garden',\n",
       "  'great',\n",
       "  'transport',\n",
       "  'links',\n",
       "  'to',\n",
       "  'central',\n",
       "  'londonall',\n",
       "  'bills',\n",
       "  'are',\n",
       "  'included',\n",
       "  'in',\n",
       "  'the',\n",
       "  'rent'],\n",
       " ['please',\n",
       "  'feel',\n",
       "  'free',\n",
       "  'to',\n",
       "  'get',\n",
       "  'in',\n",
       "  'touch',\n",
       "  'if',\n",
       "  'you',\n",
       "  'are',\n",
       "  'interested',\n",
       "  'unfortunately',\n",
       "  'we',\n",
       "  'cannot',\n",
       "  'accept',\n",
       "  'dss',\n",
       "  'pets',\n",
       "  'agents',\n",
       "  'couples',\n",
       "  'students'],\n",
       " ['a',\n",
       "  'good',\n",
       "  'size',\n",
       "  'double',\n",
       "  'room',\n",
       "  'with',\n",
       "  'plenty',\n",
       "  'of',\n",
       "  'storage',\n",
       "  'spaceavailable',\n",
       "  'in',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'areathis',\n",
       "  'room',\n",
       "  'has',\n",
       "  'large',\n",
       "  'bay',\n",
       "  'windows',\n",
       "  'which',\n",
       "  'provide',\n",
       "  'great',\n",
       "  'lighting'],\n",
       " ['large',\n",
       "  'garden',\n",
       "  'too',\n",
       "  'excellent',\n",
       "  'location',\n",
       "  'on',\n",
       "  'a',\n",
       "  'residential',\n",
       "  'road',\n",
       "  'with',\n",
       "  'free',\n",
       "  'parking'],\n",
       " ['close', 'to', 'local', 'amenities', 'such', 'as']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-845f736e4c59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, documents, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m         report_values = self.vocabulary.prepare_vocab(\n\u001b[1;32m    725\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'memory'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_retained_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mprepare_vocab\u001b[0;34m(self, hs, negative, wv, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;31m# add info about each word's Huffman encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36msort_vocab\u001b[0;34m(self, wv)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "model.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
